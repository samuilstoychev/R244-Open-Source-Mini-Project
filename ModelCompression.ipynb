{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "import numpy as np\n",
    "import tempfile\n",
    "import zipfile\n",
    "import os\n",
    "\n",
    "import tensorflow_model_optimization as tfmot\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "cluster_weights = tfmot.clustering.keras.cluster_weights\n",
    "CentroidInitialization = tfmot.clustering.keras.CentroidInitialization\n",
    "from ax.utils.notebook.plotting import render, init_notebook_plotting\n",
    "from ax.plot.contour import plot_contour\n",
    "\n",
    "import pickle\n",
    "import tikzplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** I have used TF 2.4.0 for the experiments. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Define Baseline Model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. Load MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = keras.datasets.mnist\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
    "\n",
    "# Normalize the input image so that each pixel value is between 0 to 1.\n",
    "train_images = train_images / 255.0\n",
    "test_images  = test_images / 255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. Get Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_baseline_model(): \n",
    "    \n",
    "    # Define the model architecture.\n",
    "    model = keras.Sequential([\n",
    "      keras.layers.InputLayer(input_shape=(28, 28)),\n",
    "      keras.layers.Reshape(target_shape=(28, 28, 1)),\n",
    "      keras.layers.Conv2D(filters=12, kernel_size=(3, 3), activation='relu'),\n",
    "      keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "      keras.layers.Flatten(),\n",
    "      keras.layers.Dense(10)\n",
    "    ])\n",
    "    \n",
    "    # Train the digit classification model\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    model.fit(\n",
    "      train_images,\n",
    "      train_labels,\n",
    "      epochs=5,\n",
    "      validation_split=0.1,\n",
    "    )\n",
    "    \n",
    "    return model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Compression "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. Auxiliary Functions "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.1. Get size of zipped model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_zipped_model_size(model): \n",
    "    if isinstance(model, bytes): \n",
    "        _, file = tempfile.mkstemp('.tflite')\n",
    "        with open(file, 'wb') as f:\n",
    "            f.write(model)\n",
    "    else: \n",
    "        _, file = tempfile.mkstemp('.h5')\n",
    "        tf.keras.models.save_model(model, file, include_optimizer=False)\n",
    "    print('Saved baseline model to:', file)\n",
    "    \n",
    "    _, zipped_file = tempfile.mkstemp('.zip')\n",
    "    with zipfile.ZipFile(zipped_file, 'w', compression=zipfile.ZIP_DEFLATED) as f:\n",
    "        f.write(file)\n",
    "    return os.path.getsize(zipped_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.2. Evaluate the accuracy of the model (for binaries) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(interpreter):\n",
    "    input_index = interpreter.get_input_details()[0][\"index\"]\n",
    "    output_index = interpreter.get_output_details()[0][\"index\"]\n",
    "    \n",
    "    # Run predictions on ever y image in the \"test\" dataset.\n",
    "    prediction_digits = []\n",
    "    for i, test_image in enumerate(test_images):\n",
    "        # if i % 1000 == 0:\n",
    "        #    print('Evaluated on {n} results so far.'.format(n=i))\n",
    "    \n",
    "        # Pre-processing: add batch dimension and convert to float32 to match with\n",
    "        # the model's input data format.\n",
    "        test_image = np.expand_dims(test_image, axis=0).astype(np.float32)\n",
    "        interpreter.set_tensor(input_index, test_image)\n",
    "\n",
    "        # Run inference.\n",
    "        interpreter.invoke()\n",
    "\n",
    "        # Post-processing: remove batch dimension and find the digit with highest\n",
    "        # probability.\n",
    "        output = interpreter.tensor(output_index)\n",
    "        digit = np.argmax(output()[0])\n",
    "        prediction_digits.append(digit)\n",
    "    \n",
    "    print('\\n')\n",
    "    # Compare prediction results with ground truth labels to calculate accuracy.\n",
    "    prediction_digits = np.array(prediction_digits)\n",
    "    accuracy = (prediction_digits == test_labels).mean()\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.3. Get accuracy of the model (both for Keras models and for binaries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_accuracy(model): \n",
    "    if isinstance(model, bytes): \n",
    "        interpreter = tf.lite.Interpreter(model_content=model)\n",
    "        interpreter.allocate_tensors()\n",
    "        return evaluate_model(interpreter)\n",
    "    return model.evaluate(test_images, test_labels, verbose=1)[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.2. Compression Function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_compression(baseline_model, initial_sparsity=0.5, final_sparsity=0.8, post_train_quant=False, qaware=False, clusters=0): \n",
    "    prune_low_magnitude = tfmot.sparsity.keras.prune_low_magnitude\n",
    "    \n",
    "    # Create a clone of the baseline model (to avoid various call affecting each other)\n",
    "    model = keras.models.clone_model(baseline_model)\n",
    "    model.build((None, 10)) # replace 10 with number of variables in input layer\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "                  metrics=['accuracy'])\n",
    "    model.set_weights(baseline_model.get_weights())\n",
    "    \n",
    "    # Compute end step to finish pruning after 2 epochs.\n",
    "    batch_size = 128\n",
    "    epochs = 2\n",
    "    validation_split = 0.1 # 10% of training set will be used for validation set. \n",
    "\n",
    "    num_images = train_images.shape[0] * (1 - validation_split)\n",
    "    end_step = np.ceil(num_images / batch_size).astype(np.int32) * epochs\n",
    "    \n",
    "    # Define model for pruning. \n",
    "    pruning_params = {\n",
    "          'pruning_schedule': tfmot.sparsity.keras.PolynomialDecay(initial_sparsity=initial_sparsity,\n",
    "                                                                   final_sparsity=final_sparsity,\n",
    "                                                                   begin_step=0,\n",
    "                                                                   end_step=end_step)\n",
    "    }\n",
    "\n",
    "    model_for_pruning = prune_low_magnitude(model, **pruning_params)\n",
    "\n",
    "    # `prune_low_magnitude` requires a recompile.\n",
    "    model_for_pruning.compile(optimizer='adam',\n",
    "                  loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    callbacks = [\n",
    "      tfmot.sparsity.keras.UpdatePruningStep()\n",
    "    ]\n",
    "    \n",
    "    # Note: We train with fewer weights, therefore training is faster as well. \n",
    "    model_for_pruning.fit(train_images, train_labels,\n",
    "                      batch_size=batch_size, epochs=epochs, validation_split=validation_split,\n",
    "                      callbacks=callbacks)\n",
    "    \n",
    "    # Ensure that TFLite does not affect accuracy \n",
    "    model_for_export = tfmot.sparsity.keras.strip_pruning(model_for_pruning)\n",
    "    \n",
    "    if clusters > 1:  \n",
    "        clustering_params = {\n",
    "          'number_of_clusters': clusters,\n",
    "          'cluster_centroids_init': CentroidInitialization.LINEAR\n",
    "        }\n",
    "\n",
    "        # Cluster a whole model\n",
    "        model_for_export = cluster_weights(model_for_export, **clustering_params)\n",
    "\n",
    "        # Use smaller learning rate for fine-tuning clustered model\n",
    "        # TODO: Is the learning rate also a hyperparameter? \n",
    "        opt = tf.keras.optimizers.Adam(learning_rate=1e-5)\n",
    "\n",
    "\n",
    "        model_for_export.compile(\n",
    "          loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "          optimizer=opt,\n",
    "          metrics=['accuracy'])\n",
    "\n",
    "        model_for_export.fit(\n",
    "          train_images,\n",
    "          train_labels,\n",
    "          batch_size=500,\n",
    "          epochs=1,\n",
    "          validation_split=0.1)\n",
    "        \n",
    "        model_for_export = tfmot.clustering.keras.strip_clustering(model_for_export)\n",
    "        \n",
    "    if qaware:\n",
    "        # q_aware stands for for quantization aware.\n",
    "        model_for_export = tfmot.quantization.keras.quantize_model(model_for_export)\n",
    "        model_for_export.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "        \n",
    "        train_images_subset = train_images[0:1000] # out of 60000\n",
    "        train_labels_subset = train_labels[0:1000]\n",
    "\n",
    "        model_for_export.fit(train_images_subset, train_labels_subset, batch_size=500, epochs=1, validation_split=0.1)\n",
    "        \n",
    "    # print(\"Accuracy before TFLite:\", model_for_pruning.evaluate(test_images, test_labels, verbose=1))\n",
    "    converter = tf.lite.TFLiteConverter.from_keras_model(model_for_export)\n",
    "    # TODO: Might also want to add representative samples to the post-training quantisation \n",
    "    # https://www.tensorflow.org/model_optimization/guide/quantization/post_training\n",
    "    if post_train_quant or qaware: \n",
    "        # TODO: Or this could be [tf.lite.Optimize.OPTIMIZE_FOR_SIZE] instead. \n",
    "        converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "        \n",
    "    pruned_tflite_model = converter.convert()\n",
    "    return pruned_tflite_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Bayesian Optimisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ax import optimize\n",
    "import sys\n",
    "from ax.service.ax_client import AxClient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the baseline model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_model = get_baseline_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save/Load Baseline Model\n",
    "Uncomment the cells below to save/load the baseline model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# baseline_model.save(\"baseline_models/baseline_model_1601.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_model = keras.models.load_model(\"baseline_models/baseline_model_1601.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get baseline size and accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASELINE_SIZE = get_zipped_model_size(baseline_model)\n",
    "BASELINE_ACCURACY = get_model_accuracy(baseline_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Baseline size:\", BASELINE_SIZE)\n",
    "print(\"Baseline accuracy:\", BASELINE_ACCURACY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. BO without any constraint "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax_client = AxClient()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_fun(p): \n",
    "    final_sparsity = p.get(\"final_sparsity\")\n",
    "    clusters = p.get(\"clusters\")\n",
    "    post_train_quant = p.get(\"post_train_quant\")\n",
    "    qaware = p.get(\"qaware\")\n",
    "    \n",
    "    print(final_sparsity)\n",
    "\n",
    "    accuracies = [] \n",
    "    sizes = [] \n",
    "    \n",
    "    for x in range(3): \n",
    "        res = apply_compression(baseline_model, \n",
    "                            final_sparsity=final_sparsity, \n",
    "                            clusters=clusters,\n",
    "                            post_train_quant=post_train_quant, \n",
    "                            qaware=qaware)\n",
    "        sizes.append(get_zipped_model_size(res))\n",
    "        accuracies.append(get_model_accuracy(res))\n",
    "    \n",
    "    final_size = (np.mean(sizes), np.std(sizes))\n",
    "    return {\"size\": final_size, \"accuracy\": (np.mean(accuracies), np.std(accuracies))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax_client.create_experiment(\n",
    "    name=\"compression_experiment\",\n",
    "    parameters=[\n",
    "          {\n",
    "            \"name\": \"final_sparsity\",\n",
    "            \"type\": \"range\",\n",
    "            \"value_type\": \"float\",\n",
    "            \"bounds\": [0.0, 0.999],\n",
    "          }, \n",
    "          {\n",
    "            \"name\": \"clusters\",\n",
    "            \"type\": \"range\",\n",
    "            \"value_type\": \"int\",\n",
    "            \"bounds\": [2, 100],\n",
    "          }, \n",
    "          {\n",
    "            \"name\": \"post_train_quant\",\n",
    "            \"type\": \"choice\",\n",
    "            \"value_type\": \"bool\",\n",
    "            \"values\": [True, False],\n",
    "          }, \n",
    "          {\n",
    "            \"name\": \"qaware\",\n",
    "            \"type\": \"choice\",\n",
    "            \"value_type\": \"bool\",\n",
    "            \"values\": [True, False],\n",
    "          }, \n",
    "        \n",
    "        ],\n",
    "    objective_name=\"size\",\n",
    "    minimize=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "\n",
    "for i in range(50):\n",
    "    print(\"Iteration\", i)\n",
    "    parameters, trial_index = ax_client.get_next_trial()\n",
    "    # Local evaluation here can be replaced with deployment to external system.\n",
    "    ax_client.complete_trial(trial_index=trial_index, raw_data=evaluate_fun(parameters))\n",
    "    \n",
    "print(\"TOTAL TIME TAKEN:\", time.time() - start, \"seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax_client.save_to_json_file(filepath=\"results/baseline_1601.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2. BO with `outcome_constraint`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax_client = AxClient()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_fun(p): \n",
    "    final_sparsity = p.get(\"final_sparsity\")\n",
    "    clusters = p.get(\"clusters\")\n",
    "    post_train_quant = p.get(\"post_train_quant\")\n",
    "    qaware = p.get(\"qaware\")\n",
    "\n",
    "    accuracies = [] \n",
    "    sizes = [] \n",
    "    \n",
    "    for x in range(3): \n",
    "        res = apply_compression(baseline_model, \n",
    "                            final_sparsity=final_sparsity, \n",
    "                            clusters=clusters,\n",
    "                            post_train_quant=post_train_quant, \n",
    "                            qaware=qaware)\n",
    "        sizes.append(get_zipped_model_size(res))\n",
    "        accuracies.append(get_model_accuracy(res))\n",
    "    \n",
    "    final_size = (np.mean(sizes), np.std(sizes))\n",
    "    return {\"size\": final_size, \"accuracy\": (np.mean(accuracies), np.std(accuracies))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax_client.create_experiment(\n",
    "    name=\"compression_experiment\",\n",
    "    parameters=[\n",
    "          {\n",
    "            \"name\": \"final_sparsity\",\n",
    "            \"type\": \"range\",\n",
    "            \"value_type\": \"float\",\n",
    "            \"bounds\": [0.0, 0.999],\n",
    "          }, \n",
    "          {\n",
    "            \"name\": \"clusters\",\n",
    "            \"type\": \"range\",\n",
    "            \"value_type\": \"int\",\n",
    "            \"bounds\": [2, 100],\n",
    "          }, \n",
    "          {\n",
    "            \"name\": \"post_train_quant\",\n",
    "            \"type\": \"choice\",\n",
    "            \"value_type\": \"bool\",\n",
    "            \"values\": [True, False],\n",
    "          }, \n",
    "          {\n",
    "            \"name\": \"qaware\",\n",
    "            \"type\": \"choice\",\n",
    "            \"value_type\": \"bool\",\n",
    "            \"values\": [True, False],\n",
    "          }, \n",
    "        \n",
    "        ],\n",
    "    objective_name=\"size\",\n",
    "    minimize=True,\n",
    "    outcome_constraints=[\"accuracy >= 0.95\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "\n",
    "for i in range(50):\n",
    "    print(\"Iteration\", i)\n",
    "    parameters, trial_index = ax_client.get_next_trial()\n",
    "    # Local evaluation here can be replaced with deployment to external system.\n",
    "    ax_client.complete_trial(trial_index=trial_index, raw_data=evaluate_fun(parameters))\n",
    "    \n",
    "print(\"TOTAL TIME TAKEN:\", time.time() - start, \"seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax_client.save_to_json_file(filepath=\"results/outconst_1601.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3. BO with ReturnInf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax_client = AxClient()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_fun(p): \n",
    "    final_sparsity = p.get(\"final_sparsity\")\n",
    "    clusters = p.get(\"clusters\")\n",
    "    post_train_quant = p.get(\"post_train_quant\")\n",
    "    qaware = p.get(\"qaware\")\n",
    "\n",
    "    accuracies = [] \n",
    "    sizes = [] \n",
    "    \n",
    "    for x in range(3): \n",
    "        res = apply_compression(baseline_model, \n",
    "                            final_sparsity=final_sparsity, \n",
    "                            clusters=clusters,\n",
    "                            post_train_quant=post_train_quant, \n",
    "                            qaware=qaware)\n",
    "        sizes.append(get_zipped_model_size(res))\n",
    "        accuracies.append(get_model_accuracy(res))\n",
    "    \n",
    "    if (np.mean(accuracies)) < 0.95: \n",
    "        final_size = (BASELINE_SIZE, 0)\n",
    "    else: \n",
    "        final_size = (np.mean(sizes), np.std(sizes))\n",
    "    return {\"size\": final_size, \"accuracy\": (np.mean(accuracies), np.std(accuracies))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax_client.create_experiment(\n",
    "    name=\"compression_experiment\",\n",
    "    parameters=[\n",
    "          {\n",
    "            \"name\": \"final_sparsity\",\n",
    "            \"type\": \"range\",\n",
    "            \"value_type\": \"float\",\n",
    "            \"bounds\": [0.0, 0.999],\n",
    "          }, \n",
    "          {\n",
    "            \"name\": \"clusters\",\n",
    "            \"type\": \"range\",\n",
    "            \"value_type\": \"int\",\n",
    "            \"bounds\": [2, 100],\n",
    "          }, \n",
    "          {\n",
    "            \"name\": \"post_train_quant\",\n",
    "            \"type\": \"choice\",\n",
    "            \"value_type\": \"bool\",\n",
    "            \"values\": [True, False],\n",
    "          }, \n",
    "          {\n",
    "            \"name\": \"qaware\",\n",
    "            \"type\": \"choice\",\n",
    "            \"value_type\": \"bool\",\n",
    "            \"values\": [True, False],\n",
    "          }, \n",
    "        \n",
    "        ],\n",
    "    objective_name=\"size\",\n",
    "    minimize=True,\n",
    "    outcome_constraints=[\"accuracy >= 0.95\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "\n",
    "for i in range(50):\n",
    "    print(\"Iteration\", i)\n",
    "    parameters, trial_index = ax_client.get_next_trial()\n",
    "    # Local evaluation here can be replaced with deployment to external system.\n",
    "    ax_client.complete_trial(trial_index=trial_index, raw_data=evaluate_fun(parameters))\n",
    "    \n",
    "print(\"TOTAL TIME TAKEN:\", time.time() - start, \"seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax_client.save_to_json_file(filepath=\"results/returninf_1601.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4. BO with Linear Combination "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax_client = AxClient()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_fun(p): \n",
    "    final_sparsity = p.get(\"final_sparsity\")\n",
    "    clusters = p.get(\"clusters\")\n",
    "    post_train_quant = p.get(\"post_train_quant\")\n",
    "    qaware = p.get(\"qaware\")\n",
    "\n",
    "    accuracies = [] \n",
    "    sizes = [] \n",
    "    \n",
    "    for x in range(3): \n",
    "        res = apply_compression(baseline_model, \n",
    "                            final_sparsity=final_sparsity, \n",
    "                            clusters=clusters,\n",
    "                            post_train_quant=post_train_quant, \n",
    "                            qaware=qaware)\n",
    "        sizes.append(get_zipped_model_size(res))\n",
    "        accuracies.append(get_model_accuracy(res))\n",
    "    \n",
    "    final_size = (np.mean(sizes), np.std(sizes))\n",
    "    final_accuracy = (np.mean(accuracies), np.std(accuracies))\n",
    "    linear_comb = final_size[0] + (BASELINE_ACCURACY - final_accuracy[0]) * BASELINE_SIZE\n",
    "    \n",
    "    return {\"linear_combination\": linear_comb, \"size\": final_size, \"accuracy\": final_accuracy}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax_client.create_experiment(\n",
    "    name=\"compression_experiment\",\n",
    "    parameters=[\n",
    "          {\n",
    "            \"name\": \"final_sparsity\",\n",
    "            \"type\": \"range\",\n",
    "            \"value_type\": \"float\",\n",
    "            \"bounds\": [0.0, 0.999],\n",
    "          }, \n",
    "          {\n",
    "            \"name\": \"clusters\",\n",
    "            \"type\": \"range\",\n",
    "            \"value_type\": \"int\",\n",
    "            \"bounds\": [2, 100],\n",
    "          }, \n",
    "          {\n",
    "            \"name\": \"post_train_quant\",\n",
    "            \"type\": \"choice\",\n",
    "            \"value_type\": \"bool\",\n",
    "            \"values\": [True, False],\n",
    "          }, \n",
    "          {\n",
    "            \"name\": \"qaware\",\n",
    "            \"type\": \"choice\",\n",
    "            \"value_type\": \"bool\",\n",
    "            \"values\": [True, False],\n",
    "          }, \n",
    "        \n",
    "        ],\n",
    "    objective_name=\"linear_combination\",\n",
    "    minimize=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "\n",
    "for i in range(50):\n",
    "    print(\"Iteration\", i)\n",
    "    parameters, trial_index = ax_client.get_next_trial()\n",
    "    # Local evaluation here can be replaced with deployment to external system.\n",
    "    ax_client.complete_trial(trial_index=trial_index, raw_data=evaluate_fun(parameters))\n",
    "    \n",
    "print(\"TOTAL TIME TAKEN:\", time.time() - start, \"seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax_client.save_to_json_file(filepath=\"results/lin_comb_1701.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Other Optimisation Methods "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1. Random Search "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The evaluation function remains similar: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_fun(final_sparsity, clusters, post_train_quant, qaware): \n",
    "    \n",
    "    accuracies = [] \n",
    "    sizes = [] \n",
    "    \n",
    "    for x in range(3): \n",
    "        res = apply_compression(baseline_model, \n",
    "                            final_sparsity=final_sparsity, \n",
    "                            clusters=clusters,\n",
    "                            post_train_quant=post_train_quant, \n",
    "                            qaware=qaware)\n",
    "        sizes.append(get_zipped_model_size(res))\n",
    "        accuracies.append(get_model_accuracy(res))\n",
    "    \n",
    "    final_size = (np.mean(sizes), np.std(sizes))\n",
    "    return {\"size\": final_size, \"accuracy\": (np.mean(accuracies), np.std(accuracies))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_parameters(): \n",
    "    return {\n",
    "        \"final_sparsity\": np.random.rand(), \n",
    "        \"clusters\": np.random.randint(2,101), \n",
    "        \"post_train_quant\": bool(np.random.randint(0,2)), \n",
    "        \"qaware\": bool(np.random.randint(0,2))\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "\n",
    "results = [] \n",
    "\n",
    "for i in range(55):\n",
    "    print(\"Iteration\", i)\n",
    "    parameters = get_random_parameters()\n",
    "    print(\"Parameters\", parameters)\n",
    "    res = (parameters, evaluate_fun(**parameters))\n",
    "    results.append(res)\n",
    "    \n",
    "print(\"TOTAL TIME TAKEN:\", time.time() - start, \"seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('results/random_search_results_1701.pickle', 'wb') as file:\n",
    "    pickle.dump(results, file, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2. Grid Search (Exhaustive Search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_fun(final_sparsity, clusters, post_train_quant, qaware): \n",
    "    \n",
    "    accuracies = [] \n",
    "    sizes = [] \n",
    "    \n",
    "    for x in range(3): \n",
    "        res = apply_compression(baseline_model, \n",
    "                            final_sparsity=final_sparsity, \n",
    "                            clusters=clusters,\n",
    "                            post_train_quant=post_train_quant, \n",
    "                            qaware=qaware)\n",
    "        sizes.append(get_zipped_model_size(res))\n",
    "        accuracies.append(get_model_accuracy(res))\n",
    "    \n",
    "    final_size = (np.mean(sizes), np.std(sizes))\n",
    "    return {\"size\": final_size, \"accuracy\": (np.mean(accuracies), np.std(accuracies))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "\n",
    "results = [] \n",
    "\n",
    "for final_sparsity in np.linspace(0, 0.99, num=7): \n",
    "    for clusters in range(2, 101, 30): \n",
    "        for post_train_quant in [True, False]: \n",
    "            for qaware in [True, False]: \n",
    "                parameters = [final_sparsity, clusters, post_train_quant, qaware]\n",
    "                print(\"Testing with\", parameters)\n",
    "                res = evaluate_fun(float(final_sparsity), clusters, post_train_quant, qaware)\n",
    "                results.append((parameters, res))\n",
    "                \n",
    "print(\"TOTAL TIME TAKEN:\", time.time() - start, \"seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving the Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('results/grid_search_results_1601.pickle', 'wb') as file:\n",
    "    pickle.dump(results, file, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sensitivity Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"font.size\"] = \"13\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pruning sparsity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_sizes = [] \n",
    "model_accuracies = [] \n",
    "all_models = [] \n",
    "in_sp = []\n",
    "fin_sp = []\n",
    "\n",
    "for final_sparsity in np.linspace(0.80, 0.99, 10): \n",
    "        fin_sp.append(final_sparsity)\n",
    "        print(\"final_sparsity =\", str(final_sparsity))\n",
    "        model_with_pruning = apply_compression(baseline_model, \n",
    "                                           initial_sparsity=0.5,\n",
    "                                           final_sparsity=float(final_sparsity))\n",
    "        all_models.append(model_with_pruning)\n",
    "        model_sizes.append(get_zipped_model_size(model_with_pruning))\n",
    "        model_accuracies.append(get_model_accuracy(model_with_pruning))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = np.linspace(0.80, 0.99, 10)\n",
    "data1 = model_sizes\n",
    "data2 = [x*100 for x in model_accuracies]\n",
    "\n",
    "fig, ax1 = plt.subplots(figsize=(6.5,5))\n",
    "\n",
    "color = 'tab:red'\n",
    "ax1.set_xlabel('Level of sparsity')\n",
    "ax1.set_ylabel('Model size (bytes)', color=color)\n",
    "ax1.plot(t, data1, color=color, linewidth=2)\n",
    "ax1.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "ax2 = ax1.twinx()  # instantiate a second axes that shares the same x-axis\n",
    "\n",
    "color = 'tab:blue'\n",
    "ax2.set_ylabel('Inference accuracy (%)', color=color)  # we already handled the x-label with ax1\n",
    "ax2.plot(t, data2, color=color, linewidth=2)\n",
    "ax2.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "fig.tight_layout()  # otherwise the right y-label is slightly clipped\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig.savefig(\"graphs/sparsity.svg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Number of clusters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wc_models = [] \n",
    "wc_accuracies = [] \n",
    "wc_sizes = [] \n",
    "wc_options = [2 ,5, 10, 16, 20, 25, 50]\n",
    "\n",
    "\n",
    "for n in wc_options: \n",
    "    res = apply_compression(baseline_model, initial_sparsity=0.0, final_sparsity=0.0, clusters=n)\n",
    "    wc_accuracies.append(get_model_accuracy(res))\n",
    "    wc_sizes.append(get_zipped_model_size(res))\n",
    "    wc_models.append(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = wc_options\n",
    "data1 = wc_sizes\n",
    "data2 = [x*100 for x in wc_accuracies]\n",
    "\n",
    "fig, ax1 = plt.subplots(figsize=(6.5,5))\n",
    "\n",
    "color = 'tab:red'\n",
    "ax1.set_xlabel('Number of clusters')\n",
    "ax1.set_ylabel('Model size (bytes)', color=color)\n",
    "ax1.plot(t, data1, color=color, linewidth=2)\n",
    "ax1.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "ax2 = ax1.twinx()  # instantiate a second axes that shares the same x-axis\n",
    "\n",
    "color = 'tab:blue'\n",
    "ax2.set_ylabel('Inference accuracy (%)', color=color)  # we already handled the x-label with ax1\n",
    "ax2.plot(t, data2, color=color, linewidth=2)\n",
    "ax2.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "fig.tight_layout()  # otherwise the right y-label is slightly clipped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig.savefig(\"graphs/clusters.svg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weight distribution experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_weights = [] \n",
    "for i in range(4): \n",
    "    all_weights += list(baseline_model.get_weights()[i].flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"font.size\"] = \"13\"\n",
    "plt.figure(figsize=(6,5))\n",
    "plt.hist(all_weights, bins=300)\n",
    "plt.xlabel(\"Weight value\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.savefig(\"graphs/weights.svg\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pruned_model(baseline_model, final_sparsity=0.8): \n",
    "    prune_low_magnitude = tfmot.sparsity.keras.prune_low_magnitude\n",
    "    \n",
    "    # Create a clone of the baseline model (to avoid various call affecting each other)\n",
    "    model = keras.models.clone_model(baseline_model)\n",
    "    model.build((None, 10)) # replace 10 with number of variables in input layer\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "                  metrics=['accuracy'])\n",
    "    model.set_weights(baseline_model.get_weights())\n",
    "    \n",
    "    # Compute end step to finish pruning after 2 epochs.\n",
    "    batch_size = 128\n",
    "    epochs = 2\n",
    "    validation_split = 0.1 # 10% of training set will be used for validation set. \n",
    "\n",
    "    num_images = train_images.shape[0] * (1 - validation_split)\n",
    "    end_step = np.ceil(num_images / batch_size).astype(np.int32) * epochs\n",
    "    \n",
    "    # Define model for pruning. \n",
    "    pruning_params = {\n",
    "          'pruning_schedule': tfmot.sparsity.keras.PolynomialDecay(initial_sparsity=initial_sparsity,\n",
    "                                                                   final_sparsity=final_sparsity,\n",
    "                                                                   begin_step=0,\n",
    "                                                                   end_step=end_step)\n",
    "    }\n",
    "\n",
    "    model_for_pruning = prune_low_magnitude(model, **pruning_params)\n",
    "\n",
    "    # `prune_low_magnitude` requires a recompile.\n",
    "    model_for_pruning.compile(optimizer='adam',\n",
    "                  loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    callbacks = [\n",
    "      tfmot.sparsity.keras.UpdatePruningStep()\n",
    "    ]\n",
    "    \n",
    "    # Note: We train with fewer weights, therefore training is faster as well. \n",
    "    model_for_pruning.fit(train_images, train_labels,\n",
    "                      batch_size=batch_size, epochs=epochs, validation_split=validation_split,\n",
    "                      callbacks=callbacks)\n",
    "    \n",
    "    # Ensure that TFLite does not affect accuracy \n",
    "    model_for_export = tfmot.sparsity.keras.strip_pruning(model_for_pruning)\n",
    "    return model_for_export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pruned_model = get_pruned_model(baseline_model, final_sparsity=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_weights_after_pruning = [] \n",
    "for i in range(4): \n",
    "    all_weights_after_pruning += list(pruned_model.get_weights()[i].flatten())\n",
    "all_weights_after_pruning = list(filter(lambda x: x != 0, all_weights_after_pruning))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"font.size\"] = \"13\"\n",
    "plt.figure(figsize=(6,5))\n",
    "plt.hist(all_weights_after_pruning, bins=300)\n",
    "plt.xlabel(\"Weight value\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.savefig(\"graphs/weights_after_pruning.png\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
